# -*- coding: utf-8 -*-
"""Conversation Management & Classification using Groq API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R4IFzf6tOcrgDu6moxd9HhLIW3U7CmZr
"""

# Assignment: Conversation Management & Classification using Groq API

# Step 1: Install the required library
!pip install groq -q

# Step 2: Import necessary libraries
import os
import json
from groq import Groq
from google.colab import userdata

print("Libraries installed and imported successfully.")

# Setup: Configure the Groq API Client

try:
    #  API key from Colab secrets
    GROQ_API_KEY = userdata.get('GROQ_API_KEY')
    os.environ["GROQ_API_KEY"] = GROQ_API_KEY
    print("GROQ_API_KEY has been set successfully.")
except userdata.SecretNotFoundError:
    print("ERROR: Secret 'GROQ_API_KEY' not found.")
    print("Please go to the 'Secrets' tab (key icon on the left) and add your Groq API key.")
except Exception as e:
    print(f"An error occurred: {e}")


#  Groq client (using OpenAI SDK compatibility)
#  GROQ_API_KEY environment variable.
try:
    client = Groq()
    print("Groq client initialized.")
except Exception as e:
    print(f"Failed to initialize Groq client: {e}")

# Task 1: Managing Conversation History with Summarization

print("\n" + "="*80)
print("TASK 1: CONVERSATION HISTORY MANAGEMENT & SUMMARIZATION")
print("="*80 + "\n")


class ConversationManager:
    """
    Manages conversation history with features for periodic summarization and truncation.
    """
    def __init__(self, model="llama3-8b-8192", summarize_every_k_runs=3):
        self.model = model
        self.conversation_history = []
        self.run_count = 0
        self.summarize_every_k = summarize_every_k_runs

    def add_message(self, role, content):
        """Adds a new message to the history and handles periodic summarization."""
        # Add the new message
        self.conversation_history.append({"role": role, "content": content})

        # For assistant messages
        if role == 'assistant':
            self.run_count += 1
            # Check if it's time to summarize
            if self.run_count % self.summarize_every_k == 0 and len(self.conversation_history) > 1:
                print(f"\n--- Triggering summarization after {self.run_count} runs. ---")
                self._summarize()

    def _summarize(self):
        """Internal method to summarize the current history and replace it."""

        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in self.conversation_history])


        try:
            summary_response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a conversation summarizer. Summarize the following dialogue concisely, capturing the key points and context for a future AI interaction."},
                    {"role": "user", "content": history_str}
                ],
                temperature=0.2,
            )
            summary_text = summary_response.choices[0].message.content

            # Replace the old history
            self.conversation_history = [
                {"role": "system", "content": f"Summary of previous conversation: {summary_text}"}
            ]
            print("--- Conversation summarized and history replaced. ---")

        except Exception as e:
            print(f"Could not summarize the conversation. Error: {e}")

    def get_history(self):
        """Returns the full conversation history."""
        return self.conversation_history

    def get_truncated_history_by_turns(self, n_turns):
        """Returns the last n conversation turns (user + assistant = 1 turn)."""
        # A turn consists of a user and an assistant message
        return self.conversation_history[-(n_turns * 2):]

    def get_truncated_history_by_length(self, max_chars):
        """Returns the most recent messages that fit within a character limit."""
        truncated_history = []
        current_length = 0
        for message in reversed(self.conversation_history):
            msg_len = len(message['content'])
            if current_length + msg_len > max_chars:
                break
            truncated_history.insert(0, message)
            current_length += msg_len
        return truncated_history

# Demonstration of Task 1

print("### DEMONSTRATION OF TASK 1 ###\n")
# Initialize manager to summarize after every 3rd assistant response
manager = ConversationManager(summarize_every_k_runs=3)

# Sample conversation flow
conversation_flow = [
    ("user", "Hi, I'd like to book a flight to Tokyo."),
    ("assistant", "Of course! When would you like to travel?"),
    ("user", "Sometime in the first week of October, maybe the 3rd or 4th."),
    ("assistant", "Great. And from which airport will you be departing?"),
    ("user", "I'll be flying out of San Francisco, SFO."),
    ("assistant", "Perfect. I'm searching for flights from SFO to Tokyo around October 3rd or 4th. One moment."),
    ("user", "Also, can you check for hotels near Shinjuku station?"),
    ("assistant", "Certainly. I'll check for hotel availability in Shinjuku for those dates as well."),
    ("user", "Thanks, that's all for now."),
    ("assistant", "You're welcome. I'll get back to you with flight and hotel options shortly.")
]

# Feed the conversation samples into the manager
for i, (role, content) in enumerate(conversation_flow):
    print(f"Adding message {i+1}: Role='{role}', Content='{content[:40]}...'")
    manager.add_message(role, content)
    # We print the history after each assistant message to show the progression
    if role == 'assistant':
        print(f"\n>>> Current History (Run {manager.run_count}):")
        print(json.dumps(manager.get_history(), indent=2))
        print("-" * 20)

# --- Demonstrate Truncation Options ---
print("\n### Demonstrating Truncation Options on Final History ###")

# a. Limit by number of turns (e.g., last 2 turns)
print("\n--- a. Truncated by last 2 turns ---")
truncated_by_turns = manager.get_truncated_history_by_turns(n_turns=2)
print(json.dumps(truncated_by_turns, indent=2))

# b. Limit by character length (e.g., last 150 characters)
print("\n--- b. Truncated by last 150 characters ---")
truncated_by_length = manager.get_truncated_history_by_length(max_chars=150)
print(json.dumps(truncated_by_length, indent=2))



# Task 2: JSON Schema Classification & Information Extraction
print("\n" + "="*80)
print("TASK 2: JSON SCHEMA CLASSIFICATION & INFORMATION EXTRACTION")
print("="*80 + "\n")

# 1.  JSON schema for information extraction
user_details_schema = {
    "type": "object",
    "properties": {
        "name": {
            "type": ["string", "null"],
            "description": "The full name of the user.",
        },
        "email": {
            "type": ["string", "null"],
            "description": "The user's email address.",
        },
        "phone": {
            "type": ["string", "null"],
            "description": "The user's phone number, including country code if available.",
        },
        "location": {
            "type": ["string", "null"],
            "description": "The user's city and country, e.g., 'San Francisco, USA'.",
        },
        "age": {
            "type": ["integer", "null"],
            "description": "The user's age as an integer.",
        }
    },
    "required": ["name", "email", "phone", "location", "age"],
    "additionalProperties": False # Enforces strict adherence
}

# 2. tool for the Groq API, compatible with OpenAI's function calling
extraction_tool = {
    "type": "function",
    "function": {
        "name": "extract_user_details",
        "description": "Extracts user information from the chat text and returns it in a structured JSON format. If a piece of information is not found, it should be set to null.",
        "parameters": user_details_schema
    },
}

def extract_info_from_chat(user_input):
    """
    Uses Groq's tool-calling feature to parse user input against a schema.
    """
    print(f"Parsing chat: '{user_input}'")
    try:
        response = client.chat.completions.create(
            model="llama3-70b-8192",
            messages=[
                {"role": "system", "content": "You are an information extraction assistant. Your task is to extract user details from the provided text and format it using the 'extract_user_details' tool. Only use the tool provided."},
                {"role": "user", "content": user_input}
            ],
            tools=[extraction_tool],
            # Force the model to use our function
            tool_choice={"type": "function", "function": {"name": "extract_user_details"}},
            temperature=0.0
        )


        response_message = response.choices[0].message
        if response_message.tool_calls:
            tool_call = response_message.tool_calls[0]
            if tool_call.function.name == "extract_user_details":
                # The arguments are a JSON string, so we need to parse them
                extracted_data = json.loads(tool_call.function.arguments)
                return extracted_data
        return {"error": "Tool not called by the model."}

    except Exception as e:
        return {"error": f"An API call error occurred: {e}"}

# --- Demonstration of Task 2 ---

print("### DEMONSTRATION OF TASK 2 ###\n")

# 3. Define sample chats and parse them
sample_chats = [
    "Hi, my name is Jane Doe and I'm 29. My email is jane.d@example.com and I live in New York, USA. My number is 1-800-555-1234.",
    "Sure, you can reach me at contact@test.org. I'm David Chen, based in London.",
    "I'm interested in your services. I don't want to give my details right now, just exploring."
]

for i, chat in enumerate(sample_chats):
    print(f"\n--- Parsing Chat {i+1} ---")
    extracted_info = extract_info_from_chat(chat)

    # 4. "Validation" happens by checking the output structure.
    # The API call with a strict
    print("\n>>> Extracted Information:")
    if "error" in extracted_info:
        print(f"   Error: {extracted_info['error']}")
    else:
        # Pretty print the JSON output
        print(json.dumps(extracted_info, indent=4))
        # Simple validation check
        if all(key in extracted_info for key in user_details_schema["properties"]):
            print("\n   Validation: SUCCESS - Output matches the required schema keys.")
        else:
            print("\n   Validation: FAILED - Output is missing schema keys.")

print("\n" + "="*80)
print("ASSIGNMENT COMPLETE")
print("="*80 + "\n")

